
\subsubsection{\label{sec:discussion:theory:detectionoverview}Obstacle Detection Overview}
While sailors have used navigation aides for many years, only recently has there been an interest in developing autonomous vehicles that could navigate themselves around the ocean for extended periods of time. While there is a wealth of information available on object detection for autonomous surface vehicles \cite{detection-offroad} \cite{optical-flow-detection} \cite{unmanned-ground-vehicles}, many of these techniques involve laser rangefinding or stereo vision in static, sterile environments. None of these techniques are easily applicable to an ocean environment, where the platform and environment are never static, waves and fog mean a constant air/water boundary has to be considered, salt poses a corrosive threat and marine life may grow on or distort the imaging medium. 

Autonomous underwater vehicles use forward looking sonar but the moving air/water medium for a boat on the surface makes sonar difficult to use when not completely submersed. While profiling sonar has been used in a research capacity on a surface boat\cite{ASV-sonar} the profiling system was only effective when tested on a small lake with a slow-moving boat, for objects at a distance of <30m. This case is a far cry from the conditions that the Sailbot will experience, and furthermore cost considerations rule out sonar obstacle detection systems for us.

The most applicable research we found to our work with the Lepton infrared camera was done at the Old Dominion University in Viriginia, by their Computational Intelligence and Machine Vision Labratory. The ODU lab had developed an algorithm to count boats passing into a harbour by use of an infrared camera, primarily through the use of adaptive thresholding and training a machine learning algorithm on a large database of boat images; however, their work could be fooled on even slightly wavey water, as it was designed with harbour conditions in mind. Watching the sample video on their site shows the algorithm misidentifying water as an obstacle when there are even minor waves.\cite{ODU-boat-IR-detection} Despite this, ODU's work shows that IR imaging holds promise in marine obstacle detection, and much of our work follows their model.

We must also consider the properties of the objects we're viewing. Low-floating objects will typically have a temperature near the temperature of the water, however they are visible if their emissivity is different from that of water. Liquid water has an emissivity of $.90$ to $.95$; ice has an emissivity of $.95$ to $1$, making it appear white in an IR image against a water background; and wood has an emissivity of $.75$ to $.95$, making it appear black against water. Rain or fog creates a barrier between the camera and any objects being viewed; water droplets scatter IR light a great degree, so image quality is expected to degrade during heav percipitation. Additionally, on a bright day any clouds will appear bright and stark against the sky; filtering out objects above the horizon will be very important. \cite{optotherm-emisstable}

\subsubsection{\label{sec:discussion:theory:detectionsoftware}Software}
Acquiring a usable image is only the first step in object detection. In order to train the software to see the objects in the frame, we apply many of the basic algorithms that the human brain uses to identify and partition objects. Our eyes are developed to look for sharp lines and movement against a background; much of the work in machine vision focuses on image segmentation and object tracking, similarly. 

In the ideal detection case, we would see a large boat on still water, with a clear sky. In this ideal image, the only sharp lines are those which define the boat and the horizon, at which point linefinding is easy and detection is trivial. However, the imaging conditions that Sailbot will experience are sure to be far from ideal; waves create varying angles of incidence, meaning a varying IR intensity across uniform-temperature water, the boats may be small and far away, and the Sailbot won't ever be still. This means that image smoothing must be performed; filters must be in place to help segment the image and discard noise. While basic smoothing can take place as simple blurring (see Figure \ref{fig:blurring:sub1}, an example from the O'Rielly OpenCV textbook on computer vision), simple techniques such as mean or median blurring (where the pixel value is set to the mean/median of those around it) is a highly lossy technique. Gaussian smoothing (or bilateral smoothing, which is Gaussian with a bias towards similarly-coloured pixels) is much better at preserving edges while smoothing low frequency noise (see Figure \ref{fig:blurring:sub2}).

\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
 \centering
 \includegraphics[width=0.95\textwidth]{"./image/median_blurring"}
 \caption{Blurring by taking the median of the surrounding pixels.}
 \label{fig:blurring:sub1}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
 \centering
 \includegraphics[width=0.95\textwidth]{"./image/bilateral_smoothing"}
 \caption{Bilateral smoothing; applying a weighted Gaussian smoothing.}
 \label{fig:blurring:sub2}
\end{subfigure}
\caption{Two blurring techniques to cut down noise}
\label{fig:blurring}
\end{figure}

The FLIR Lepton camera we're using for this project is a very low resolution camera -- only 80x60 in a 50 degree field of view -- so smoothing is not really needed; each pixel covers several square feet of most objects it's looking at, so the images look like they've been through a heavy mean filtering process already (Figure \ref{fig:buoy_ex}. However, should the Sailbot upgrade to a higher resolution camera (as is being currently considered) then smoothing algorithms wwill play a more important role in our postprocessing. 

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{"./image/buoy_example"}
\caption{Buoy at 50 meters, no smoothing or processing}
\label{fig:buoy_ex}
\end{figure}

After reducing noise, increasing contrast and thresholding help to segment the image. In infrared scenes captured during rain or through heavy fog, much of the IR light will be scattered resulting in washed-out pictures (see Figure \ref{fig:washedOut} taken at midafternoon on a foggy day of testing in Ucluelet). When the obstacles are close, the contrast in the background is not a problem as the object sticks out quite obviously. However, far-off objects will have less contrast, where applying a histogram equilization helps to increase the range of pixels used. Figure \ref{fig:hist_equil} illustrates the effect that the histogram equilization can have; by mapping the histogram of pixel intensity values to a cumulative Gaussian distribution, the full range of pixels can be utilized. In scenes with little or no information (such as that in Figure \ref{fig:washedOut}) gain no net benefit from increasing contrast.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{"./image/washed-out_example"}
\caption{Boat at 100m to 150m through fog}
\label{fig:washedOut}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
 \centering
 \includegraphics[width=0.95\textwidth]{"./image/hist_equil_1"}
 \caption{Low contrast example.}
 \label{fig:hist_equil:sub1}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
 \centering
 \includegraphics[width=0.95\textwidth]{"./image/hist_equil_2"}
 \caption{Post-processing.}
 \label{fig:hist_equil:sub2}
\end{subfigure}
\caption{Increasing contrast with histogram equilization.}
\label{fig:hist_equil}
\end{figure}

The next step in most traditional object detection algorithms is to apply a thresholding technique: blocking off all pixels less than a certain intensity value is an easy way to prioritize close, large, easy-to-identify objects. Particularly with infrared imaging, the pixel intensity corresponding to an object is proportional to its heat; in the ocean, boats are drastically warmer than the ocean surrounding them. In addition, a ceiling threshold helps to eliminate the sun from detection as a potential obstacle, as the recorded intensity of the sun is orders of magnitude higher than those of most any terrestrial objects. Rather than apply a static threshold, as the recorded pixel values can vary by large amounts depending on the time of day, position of the sun, cloud cover, and fog, an adaptive threshold is much more useful. Adaptive thresholds are "smarter" thresholds, given that they take into account the overall pixel intensities in the image and apply a differing threshold to each part of the image based on its surroundings. Adaptive thresholds excel at accounting for lighting differences, but in our application they can help account for sun position and waves. Figure \ref{fig:thresh_comp} illustrates the benefit of adaptive thresholds over traditional binary thresholds.

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{"./image/threshold_comparisons"}
\caption{Simple vs Adaptive Thresholding.}
\label{fig:thresh_comp}
\end{figure}

Finally, after as much extraneous information has been removed as possible, edge detection algorithms can be applied to find areas of complex geometry in the image. The most commonplace edge detection algorithm is Canny edge detection, which maps the first and second derivatives in both horizontal and vertical directions over the image before looking for local maxima and minima; by pairing these extrema with a gradient analysis and applying a threshold to filter for high rates of change, the Canny algorithm is very effective at finding edges. In the Sailbot case, we can use Canny edge detection on our filtered images to identify areas with many edges. 
\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{"./image/canny"}
\caption{Canny edge detection.}
\label{fig:canny}
\end{figure}

High concentrations of edges combined with lines at sharp angles to the horizon have a high likelihood to be an obstacle, while very rarely being triggered by ocean waves or other noise. Since obtacles prove to be persistent where noise doesn't trip the algorithm often enough, weighting an obstacle's likelihood of existence by it's frequency of appearance is a strong method to filter out false positives while limiting the chance of false negatives.


\subsubsection{\label{sec:discussion:theory:righardware}Hardware}

The rig containing the IR camera will be exposed to a heavily corrosive, destructive environment when travelling across the Atlantic ocean. Salt water will evaporate to leave deposits on all surfaces of the equipment, heavy waves will be assumed to crash over it on a semi-regular basis, and marine life could pose additional problems. In order to alleviate these problems, the rig must be both strong and waterproof; several theoretical considerations must be taken into account during the design.

Firstly, the lens of the camera must be strong and water resistant. The initial version of the test rig used a thin plastic film as the lens; while the HDPE film had excellent long-wave IR transparency, this is not a suitable solution for a long-term, durable build. Many materials were considered for the lens of the final rig design, with rigidity and transparency in the 7-13 micron range being the most important factors. While certain materials such as sapphire crystals are often used in long-wave infrared imaging, most are too prone to scratching or, worst of all, are water soluble. An approprite solution was found with Zinc Selenide, which has excellent, even transmission across the entire spectrum our camera operates in (Figure \ref{fig:spectra}).

\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
 \centering
 \includegraphics[width=0.95\textwidth]{"./image/zinc_selenide_spectra"}
 \caption{Zinc Selenide transmission spectra.}
 \label{fig:spectra:sub1}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
 \centering
 \includegraphics[width=0.95\textwidth]{"./image/edmund_plastic_spectra"}
 \caption{Typical IR-transparent plastic spectra.}
 \label{fig:spectra:sub2}
\end{subfigure}
\caption{Comparing transmission spectra of various materials.}
\label{fig:spectra}
\end{figure}

Secondly, any water that sits on the lens of our camera will evaporate to leave behind salt deposits, obscuring and degrading our image quality significantly over time. While not a robust subject of research, anecdotal evidence from several boat owners tells us that within two weeks at sea, any unattended surface becomes heavily encrusted. So, any method of reducing salt deposits is highly desired. 

Hydrophobic coatings seem to be an obvious choice; if water never stays on the lens, then it can't evaporate there. However, there are no commercial coatings built with the intent to remain IR transparent. We found several basic spray-on coatings, such as Rain-X or the like, to be minimally detrimental to the transmission spectra of our lens but these coatings have a short lifetime; they're meant to be reapplied every week or two (something clearly impossible on the Sailbot). While one of these coatings will be applied at the beginning of the voyage, it can only be assumed to be useful for a few days after launch.

A second, less beneficial choice comes in the form of a passive system. Similar to the aerodynamics of bug deflectors on flat-front vehicles, \cite{bug-deflector} by disrupting the airflow in front of the lens in a manner so that the air flows sharply upwards in front of it, a sort of "air wall" helps to intercept airborne droplets and move them past the lens before they land. As the Sailbot's expected to be operating in areas of high wind and travelling at speed, this effect may significantly reduce the amount of water droplets that land on the lens without consuming any power or adding any significant weight or complexity to the system. Wind tunnel testing is planned for our model in late April.
